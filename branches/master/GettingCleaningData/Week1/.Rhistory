library(rpart)
library(rpart.plot)
# install.packages("pROC")
library("pROC")
# install.packages("kernlab")
library(kernlab)
# install.packages("neuralnet")
library(neuralnet)
#
afinn_list <- read.delim("~/R/MITx/Kaggle Competition/AFINN-111.txt", header=FALSE, stringsAsFactors=FALSE)
View(afinn_list)
library(plyr)
library(stringr)
library(e1071)
colnames(afinn_list) = c('word','score')
#categorize words as very negative to very positive and add some movie-specific words
vNegTerms <- afinn_list$word[afinn_list$score==-5 | afinn_list$score==-4]
negTerms <- c(afinn_list$word[afinn_list$score==-3 | afinn_list$score==-2 | afinn_list$score==-1])
posTerms <- c(afinn_list$word[afinn_list$score==3 | afinn_list$score==2 | afinn_list$score==1])
vPosTerms <- c(afinn_list$word[afinn_list$score==5 | afinn_list$score==4], "uproarious", "riveting", "fascinating", "dazzling", "legendary")
#function to calculate number of words in each category within a sentence
sentimentScore <- function(sentences, vNegTerms, negTerms, posTerms, vPosTerms){
final_scores <- matrix('', 0, 5)
scores <- laply(sentences, function(sentence, vNegTerms, negTerms, posTerms, vPosTerms){
initial_sentence <- sentence
#remove unnecessary characters and split up by word
sentence <- gsub('[[:punct:]]', '', sentence)
sentence <- gsub('[[:cntrl:]]', '', sentence)
sentence <- gsub('\\d+', '', sentence)
sentence <- tolower(sentence)
wordList <- str_split(sentence, '\\s+')
words <- unlist(wordList)
#build vector with matches between sentence and each category
vPosMatches <- match(words, vPosTerms)
posMatches <- match(words, posTerms)
vNegMatches <- match(words, vNegTerms)
negMatches <- match(words, negTerms)
#sum up number of words in each category
vPosMatches <- sum(!is.na(vPosMatches))
posMatches <- sum(!is.na(posMatches))
vNegMatches <- sum(!is.na(vNegMatches))
negMatches <- sum(!is.na(negMatches))
score <- c(vNegMatches, negMatches, posMatches, vPosMatches)
#add row to scores table
newrow <- c(initial_sentence, score)
final_scores <- rbind(final_scores, newrow)
return(final_scores)
}, vNegTerms, negTerms, posTerms, vPosTerms)
return(scores)
}
sentences = as.character(News$Headline)
HL.Sentiment = as.data.frame( sentimentScore(sentences, vNegTerms, negTerms, posTerms, vPosTerms))
colnames(HL.Sentiment) = c('HL','HLvNegTerms', 'HLnegTerms', 'HLposTerms', 'HLvPosTerms')
sentences = as.character(News$Abstract)
AB.Sentiment = as.data.frame( sentimentScore(sentences, vNegTerms, negTerms, posTerms, vPosTerms))
colnames(AB.Sentiment) = c('AB','ABvNegTerms', 'ABnegTerms', 'ABposTerms', 'ABvPosTerms')
# Combine the two
blogsentiment = cbind(HL.Sentiment, AB.Sentiment)
blogsentiment$HL = NULL
blogsentiment$AB = NULL
# Combine sentiment and News
News = cbind(News, blogsentiment)
str(News)
View(News)
News$PubDate = NULL
View(News)
News$Headline = NULL
News$Abstract = NULL
View(News)
News$Sex = NULL
News$NY = NULL
str(News)
# Make Train & Test
Train = News[News$isTest==0,]
Test = News[News$isTest==1,]
Train$isTest=NULL
Train$UniqueID=NULL
Test$isTest=NULL
Test$Popular = NULL
View(Test)
View(Train)
View(NewsTest)
View(Test)
View(Train)
View(Test)
Test$row.names = NULL
View(Test)
str(Test)
myglm = glm(Popular ~ ., data=Train, family=binomial)
summary(myglm)
pargs3 = Popular~.
#-----------------------------------------
library(doParallel)
# SET UP THE PARAMETER SPACE SEARCH GRID
ctrl <- trainControl(method="repeatedcv",          # use repeated 10fold cross validation
repeats=5,	                        # do 5 repititions of 10-fold cv
summaryFunction=twoClassSummary,	# Use AUC to pick the best model
classProbs=TRUE)
# Note that the default search grid selects 3 values of each tuning parameter
#
grid <- expand.grid(.interaction.depth = seq(1,7,by=2), # look at tree depths from 1 to 7
.n.trees=seq(10,100,by=5),	        # let iterations go from 10 to 100
.shrinkage=c(0.01,0.1))		# Try 2 values of the learning rate parameter
# BOOSTED TREE MODEL
set.seed(123)
registerDoParallel(4)		                        # Registrer a parallel backend for train
getDoParWorkers()
system.time(gbm.tune <- train(pargs3,
data = Train,
method = "gbm",
metric = "ROC",
trControl = ctrl,
tuneGrid=grid,
verbose=FALSE))
#---------------------------------
# SUPPORT VECTOR MACHINE MODEL
#
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
svm.tune <- train(pargs3,
data = Train,
method = "svmRadial",
tuneLength = 9,		# 9 values of the cost function
preProc = c("center","scale"),
metric="ROC",
trControl=ctrl)	        # same as for gbm above
)
#---------------------------------
# RANDOM FOREST MODEL
#
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
rf.tune <- train(pargs3,
data = Train,
method = "rf",
nodesize = 5,
ntree=500,
preProc = c("center","scale"),
metric="ROC",
trControl=ctrl)	        # same as for gbm above
)
#---------------------------------
# PCR MODEL
#
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
pcr.tune <- train(pargs3,
data = Train,
method = "pcr",
tuneLength=20,
preProc = c("pca"),
metric="ROC",
trControl=ctrl)          # same as for gbm above
)
#---------------------------------
# glm MODEL
#
#
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
glm.tune = train(pargs3,
data=Train,
method="glm",
family="binomial",
preProcess = c('center', 'scale'),
metric="ROC",
trControl=ctrl)
)
#-----------------------------------
# COMPARE MODELS USING RESAPMLING
# Having set the seed to 1 before running gbm.tune and svm.tune we have generated paired samplesfor comparing models using resampling.
#
# The resamples function in caret collates the resampling results from the two models
rValues <- resamples(list(svm=svm.tune,gbm=gbm.tune, rf=rf.tune, glm=glm.tune, pca=pcr.tune))
rValues$values
#---------------------------------------------
# BOXPLOTS COMPARING RESULTS
bwplot(rValues,metric="ROC")		# boxplot
# Predict
gbm.tune.pred = predict.train(gbm.tune, newdata=Test, type="prob")
gbm.tune.pred.prob = gbm.tune.pred[,2]
rf.tune.pred = predict.train(rf.tune, newdata=Test, type="prob")
rf.tune.pred.prob = rf.tune.pred[,2]
glm.tune.pred = predict.train(glm.tune, newdata=Test, type="prob")
glm.tune.pred.prob = glm.tune.pred[,2]
svm.tune.pred = predict.train(svm.tune, newdata=Test, type="prob")
svm.tune.pred.prob = svm.tune.pred[,2]
pcr.tune.pred = predict.train(pcr.tune, newdata=Test, type="prob")
pcr.tune.pred.prob = pcr.tune.pred[,2]
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
glm.tune = train(pargs3,
data=Train,
method="glm",
family="binomial",
preProcess = c('center', 'scale'),
metric="ROC",
trControl=ctrl)
)
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
glm.tune = train(pargs3,
data=Train,
method="glm",
family="binomial",
preProcess = c('center', 'scale'),
metric="ROC",
trControl=ctrl)
rValues <- resamples(list(svm=svm.tune,gbm=gbm.tune, rf=rf.tune, glm=glm.tune, pca=pcr.tune))
rValues$values
#---------------------------------------------
# BOXPLOTS COMPARING RESULTS
bwplot(rValues,metric="ROC")		# boxplot
pcr.tune.pred = predict.train(pcr.tune, newdata=Test, type="prob")
pcr.tune.pred.prob = pcr.tune.pred[,2]
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
pcr.tune <- train(pargs3,
data = Train,
method = "pcr",
tuneLength=20,
preProc = c("pca"),
metric="ROC",
trControl=ctrl)
)
rValues <- resamples(list(svm=svm.tune,gbm=gbm.tune, rf=rf.tune, glm=glm.tune, knn=knn.tune))
rValues$values
#---------------------------------------------
# BOXPLOTS COMPARING RESULTS
bwplot(rValues,metric="ROC")		# boxplot
# Predict
gbm.tune.pred = predict.train(gbm.tune, newdata=Test, type="prob")
gbm.tune.pred.prob = gbm.tune.pred[,2]
rf.tune.pred = predict.train(rf.tune, newdata=Test, type="prob")
rf.tune.pred.prob = rf.tune.pred[,2]
glm.tune.pred = predict.train(glm.tune, newdata=Test, type="prob")
glm.tune.pred.prob = glm.tune.pred[,2]
svm.tune.pred = predict.train(svm.tune, newdata=Test, type="prob")
svm.tune.pred.prob = svm.tune.pred[,2]
xGBM = .75
xRF = .25
xGLM = .0
xSVM = .0
xKNN = .0
Probability1 = (gbm.tune.pred.prob*xGBM+
rf.tune.pred.prob*xRF+
glm.tune.pred.prob*xGLM+
svm.tune.pred.prob*xSVM+
knn.tune.pred.prob*xKNN
)
MySubmission.Tuned = data.frame(UniqueID = NewsTest$UniqueID, Probability1)
write.csv(MySubmission.Tuned, "SubmissionTuned.csv", row.names=FALSE)
gbm.tune.pred = predict(gbm.tune, newdata=Test, type="prob")
gbm.tune.pred.prob = gbm.tune.pred[,2]
gbm.tune.pred = predict(gbm.tune, newdata=Test, type="prob")
gbm.tune.pred.prob = gbm.tune.pred[,2]
rf.tune.pred = predict(rf.tune, newdata=Test, type="prob")
rf.tune.pred.prob = rf.tune.pred[,2]
glm.tune.pred = predict.train(glm.tune, newdata=Test, type="prob")
glm.tune.pred.prob = glm.tune.pred[,2]
svm.tune.pred = predict.train(svm.tune, newdata=Test, type="prob")
svm.tune.pred.prob = svm.tune.pred[,2]
xGBM = .99
xRF = .01
xGLM = .0
xSVM = .0
xKNN = .0
Probability1 = (gbm.tune.pred.prob*xGBM+
rf.tune.pred.prob*xRF+
glm.tune.pred.prob*xGLM+
svm.tune.pred.prob*xSVM+
knn.tune.pred.prob*xKNN
)
MySubmission.Tuned = data.frame(UniqueID = NewsTest$UniqueID, Probability1)
write.csv(MySubmission.Tuned, "SubmissionTuned.csv", row.names=FALSE)
xGBM = .32
xRF = .29
xGLM = .12
xSVM = .27
xKNN = .0
Probability1 = (gbm.tune.pred.prob*xGBM+
rf.tune.pred.prob*xRF+
glm.tune.pred.prob*xGLM+
svm.tune.pred.prob*xSVM+
knn.tune.pred.prob*xKNN
)
MySubmission.Tuned = data.frame(UniqueID = NewsTest$UniqueID, Probability1)
write.csv(MySubmission.Tuned, "SubmissionTuned.csv", row.names=FALSE)
xGBM = .45
xRF = .55
xGLM = .00
xSVM = .00
xKNN = .0
Probability1 = (gbm.tune.pred.prob*xGBM+
rf.tune.pred.prob*xRF+
glm.tune.pred.prob*xGLM+
svm.tune.pred.prob*xSVM+
knn.tune.pred.prob*xKNN
)
MySubmission.Tuned = data.frame(UniqueID = NewsTest$UniqueID, Probability1)
write.csv(MySubmission.Tuned, "SubmissionTuned.csv", row.names=FALSE)
install.packages("data.table")
install.packages("swirl")
swirl
library("swirl")
swirl
setwd("~/R/JohnsHopkins/GettingCleaningData")
file.exists("Week1")
dir.create("Week1")
setwd("~/R/JohnsHopkins/GettingCleaningData/Week1")
fileUrl = "https://data.baltimorecity.gove/api/views/dz54-2are/rows.csv?accessType=DOWNLOAD"
dir.create("data")
download.file(fileURL, destfile = "./data/cameras.csv", method = "curl")
list.files("./data")
download.file(fileUrl, destfile = "./data/cameras.csv", method = "curl")
list.files("./data")
install.packages("curl")
library("curl")
download.file(fileUrl, destfile = "./data/cameras.csv", method = "curl")
list.files("./data")
fileUrl = "https://data.baltimorecity.gov/api/views/dz54-2are/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl, destfile = "./data/cameras.csv", method = "curl")
list.files("./data")
download.file(fileUrl, destfile = "./data/cameras.csv")
list.files("./data")
download.file(fileUrl, destfile = "./data/cameras.csv", method="wget")
fileUrl = "https://data.baltimorecity.gov/api/views/dz54-2ar/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl, destfile = "./data/cameras.csv", method="wget")
list.files("./data")
fileUrl = "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl, destfile = "./data/cameras.csv", method="wget")
list.files("./data")
cameraData = read.table("./data/cameras.csv", sep=",", header=TRUE)
head(cameraData)
install.packages("XML")
install.packages("jsonlite")
library(data.table)
fileUrl = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
# dir.create("data")
download.file(fileUrl, destfile = "./data/properties.csv", method="wget")
list.files("./data")
propertiesData = read.table("./data/properties.csv", sep=",", header=TRUE)
Above100k = subset(propertiesData, VAL > 13)
Above1MIL = subset(propertiesData, VAL > 23)
summary("propertiesData")
des("propertiesData")
fileUrl = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileUrl, destfile = "./data/FDATA_gov_NGAP.xlsx", method="wget")
list.files("./data")
library(xlsx)
# specify the rows and columns to import
rowIndex <- 18:23
colIndex <- 7:15
# read the xlsx file
dat <- read.xlsx("./data/FDATA_gov_NGAP.xlsx", sheetIndex=1, rowIndex = rowIndex, colIndex = colIndex)
install.packages("xlsx")
library(xlsx)
# specify the rows and columns to import
rowIndex <- 18:23
colIndex <- 7:15
# read the xlsx file
dat <- read.xlsx("./data/FDATA_gov_NGAP.xlsx", sheetIndex=1, rowIndex = rowIndex, colIndex = colIndex)
install.packages("XLConnect")
install.packages("xlsxjars")
install.packages("rJava")
fileUrl = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
download.file(fileUrl, destfile = "./data/restaurants.xml", method="wget")
list.files("./data")
library(XML)
# read the XML file
doc <- xmlTreeParse('restaurants.xml', useInternalNodes = TRUE)
# define the rootnode
rootNode <- xmlRoot(doc)
# write the zipcode data to a list
zipcode <- xpathSApply(rootNode, "//zipcode", xmlValue)
# table the zipcodes
table(zipcode)[['21231']]
install.packages("XML")
library(XML)
# read the XML file
doc <- xmlTreeParse('.data/restaurants.xml', useInternalNodes = TRUE)
# define the rootnode
rootNode <- xmlRoot(doc)
# write the zipcode data to a list
zipcode <- xpathSApply(rootNode, "//zipcode", xmlValue)
# table the zipcodes
table(zipcode)[['21231']]
install.packages("xml2")
library(XML2)
install.packages("XML")
library(XML)
# read the XML file
doc <- xmlTreeParse('.data/restaurants.xml', useInternalNodes = TRUE)
# define the rootnode
rootNode <- xmlRoot(doc)
# write the zipcode data to a list
zipcode <- xpathSApply(rootNode, "//zipcode", xmlValue)
# table the zipcodes
table(zipcode)[['21231']]
download.file(fileUrl, destfile = "./data/restaurants.xml", method="curl")
list.files("./data")
# load the XML package
library(XML)
# read the XML file
doc <- xmlTreeParse('.data/restaurants.xml', useInternalNodes = TRUE)
# define the rootnode
rootNode <- xmlRoot(doc)
# write the zipcode data to a list
zipcode <- xpathSApply(rootNode, "//zipcode", xmlValue)
# table the zipcodes
table(zipcode)[['21231']]
fileUrl = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
download.file(fileUrl, destfile = "./data/restaurants.xml")
list.files("./data")
# load the XML package
library(XML)
download.file(fileUrl, destfile = "./data/restaurants.xml", method='curl')
list.files("./data")
# load the XML package
library(XML)
download.file(fileUrl, destfile = "./data/restaurants.xml", method='wget')
list.files("./data")
# load the XML package
library(XML)
doc <- xmlTreeParse('.data/restaurants.xml', useInternalNodes = TRUE)
fileUrl = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrl, destfile = "./data/Fss06pid.csv", method='wget')
list.files("./data")
# load the data.table package
library(data.table)
# read the data
DT <- fread('./data/Fss06pid.csv')
# time the processes
system.time(mean(DT$pwgtp15,by=DT$SEX))
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
system.time(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
system.time(mean(DT$pwgtp15,by=DT$SEX))
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(mean(DT$pwgtp15,by=DT$SEX))
system.time(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
system.time(mean(DT$pwgtp15,by=DT$SEX))
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
?system.time
mean(DT$pwgtp15,by=DT$SEX)
sapply(split(DT$pwgtp15,DT$SEX),mean)
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
mean(DT$pwgtp15,by=DT$SEX)
sapply(split(DT$pwgtp15,DT$SEX),mean)
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
tapply(DT$pwgtp15,DT$SEX,mean)
DT[,mean(pwgtp15),by=SEX]
cls
cl
clr
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
mean(DT$pwgtp15,by=DT$SEX)
sapply(split(DT$pwgtp15,DT$SEX),mean)
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
tapply(DT$pwgtp15,DT$SEX,mean)
DT[,mean(pwgtp15),by=SEX]
sapply(split(DT$pwgtp15,DT$SEX),mean)
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
tapply(DT$pwgtp15,DT$SEX,mean)
DT[,mean(pwgtp15),by=SEX]
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
#system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(DT[,mean(pwgtp15),by=SEX])
ver
vers
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
Rver
