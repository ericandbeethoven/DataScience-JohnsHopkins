blogsentiment = cbind(HL.Sentiment, AB.Sentiment)
blogsentiment$HL = NULL
blogsentiment$AB = NULL
# Combine sentiment and News
News = cbind(News, blogsentiment)
str(News)
View(News)
News$PubDate = NULL
View(News)
News$Headline = NULL
News$Abstract = NULL
View(News)
News$Sex = NULL
News$NY = NULL
str(News)
# Make Train & Test
Train = News[News$isTest==0,]
Test = News[News$isTest==1,]
Train$isTest=NULL
Train$UniqueID=NULL
Test$isTest=NULL
Test$Popular = NULL
View(Test)
View(Train)
View(NewsTest)
View(Test)
View(Train)
View(Test)
Test$row.names = NULL
View(Test)
str(Test)
myglm = glm(Popular ~ ., data=Train, family=binomial)
summary(myglm)
pargs3 = Popular~.
#-----------------------------------------
library(doParallel)
# SET UP THE PARAMETER SPACE SEARCH GRID
ctrl <- trainControl(method="repeatedcv",          # use repeated 10fold cross validation
repeats=5,	                        # do 5 repititions of 10-fold cv
summaryFunction=twoClassSummary,	# Use AUC to pick the best model
classProbs=TRUE)
# Note that the default search grid selects 3 values of each tuning parameter
#
grid <- expand.grid(.interaction.depth = seq(1,7,by=2), # look at tree depths from 1 to 7
.n.trees=seq(10,100,by=5),	        # let iterations go from 10 to 100
.shrinkage=c(0.01,0.1))		# Try 2 values of the learning rate parameter
# BOOSTED TREE MODEL
set.seed(123)
registerDoParallel(4)		                        # Registrer a parallel backend for train
getDoParWorkers()
system.time(gbm.tune <- train(pargs3,
data = Train,
method = "gbm",
metric = "ROC",
trControl = ctrl,
tuneGrid=grid,
verbose=FALSE))
#---------------------------------
# SUPPORT VECTOR MACHINE MODEL
#
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
svm.tune <- train(pargs3,
data = Train,
method = "svmRadial",
tuneLength = 9,		# 9 values of the cost function
preProc = c("center","scale"),
metric="ROC",
trControl=ctrl)	        # same as for gbm above
)
#---------------------------------
# RANDOM FOREST MODEL
#
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
rf.tune <- train(pargs3,
data = Train,
method = "rf",
nodesize = 5,
ntree=500,
preProc = c("center","scale"),
metric="ROC",
trControl=ctrl)	        # same as for gbm above
)
#---------------------------------
# PCR MODEL
#
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
pcr.tune <- train(pargs3,
data = Train,
method = "pcr",
tuneLength=20,
preProc = c("pca"),
metric="ROC",
trControl=ctrl)          # same as for gbm above
)
#---------------------------------
# glm MODEL
#
#
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
glm.tune = train(pargs3,
data=Train,
method="glm",
family="binomial",
preProcess = c('center', 'scale'),
metric="ROC",
trControl=ctrl)
)
#-----------------------------------
# COMPARE MODELS USING RESAPMLING
# Having set the seed to 1 before running gbm.tune and svm.tune we have generated paired samplesfor comparing models using resampling.
#
# The resamples function in caret collates the resampling results from the two models
rValues <- resamples(list(svm=svm.tune,gbm=gbm.tune, rf=rf.tune, glm=glm.tune, pca=pcr.tune))
rValues$values
#---------------------------------------------
# BOXPLOTS COMPARING RESULTS
bwplot(rValues,metric="ROC")		# boxplot
# Predict
gbm.tune.pred = predict.train(gbm.tune, newdata=Test, type="prob")
gbm.tune.pred.prob = gbm.tune.pred[,2]
rf.tune.pred = predict.train(rf.tune, newdata=Test, type="prob")
rf.tune.pred.prob = rf.tune.pred[,2]
glm.tune.pred = predict.train(glm.tune, newdata=Test, type="prob")
glm.tune.pred.prob = glm.tune.pred[,2]
svm.tune.pred = predict.train(svm.tune, newdata=Test, type="prob")
svm.tune.pred.prob = svm.tune.pred[,2]
pcr.tune.pred = predict.train(pcr.tune, newdata=Test, type="prob")
pcr.tune.pred.prob = pcr.tune.pred[,2]
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
glm.tune = train(pargs3,
data=Train,
method="glm",
family="binomial",
preProcess = c('center', 'scale'),
metric="ROC",
trControl=ctrl)
)
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
glm.tune = train(pargs3,
data=Train,
method="glm",
family="binomial",
preProcess = c('center', 'scale'),
metric="ROC",
trControl=ctrl)
rValues <- resamples(list(svm=svm.tune,gbm=gbm.tune, rf=rf.tune, glm=glm.tune, pca=pcr.tune))
rValues$values
#---------------------------------------------
# BOXPLOTS COMPARING RESULTS
bwplot(rValues,metric="ROC")		# boxplot
pcr.tune.pred = predict.train(pcr.tune, newdata=Test, type="prob")
pcr.tune.pred.prob = pcr.tune.pred[,2]
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
pcr.tune <- train(pargs3,
data = Train,
method = "pcr",
tuneLength=20,
preProc = c("pca"),
metric="ROC",
trControl=ctrl)
)
rValues <- resamples(list(svm=svm.tune,gbm=gbm.tune, rf=rf.tune, glm=glm.tune, knn=knn.tune))
rValues$values
#---------------------------------------------
# BOXPLOTS COMPARING RESULTS
bwplot(rValues,metric="ROC")		# boxplot
# Predict
gbm.tune.pred = predict.train(gbm.tune, newdata=Test, type="prob")
gbm.tune.pred.prob = gbm.tune.pred[,2]
rf.tune.pred = predict.train(rf.tune, newdata=Test, type="prob")
rf.tune.pred.prob = rf.tune.pred[,2]
glm.tune.pred = predict.train(glm.tune, newdata=Test, type="prob")
glm.tune.pred.prob = glm.tune.pred[,2]
svm.tune.pred = predict.train(svm.tune, newdata=Test, type="prob")
svm.tune.pred.prob = svm.tune.pred[,2]
xGBM = .75
xRF = .25
xGLM = .0
xSVM = .0
xKNN = .0
Probability1 = (gbm.tune.pred.prob*xGBM+
rf.tune.pred.prob*xRF+
glm.tune.pred.prob*xGLM+
svm.tune.pred.prob*xSVM+
knn.tune.pred.prob*xKNN
)
MySubmission.Tuned = data.frame(UniqueID = NewsTest$UniqueID, Probability1)
write.csv(MySubmission.Tuned, "SubmissionTuned.csv", row.names=FALSE)
gbm.tune.pred = predict(gbm.tune, newdata=Test, type="prob")
gbm.tune.pred.prob = gbm.tune.pred[,2]
gbm.tune.pred = predict(gbm.tune, newdata=Test, type="prob")
gbm.tune.pred.prob = gbm.tune.pred[,2]
rf.tune.pred = predict(rf.tune, newdata=Test, type="prob")
rf.tune.pred.prob = rf.tune.pred[,2]
glm.tune.pred = predict.train(glm.tune, newdata=Test, type="prob")
glm.tune.pred.prob = glm.tune.pred[,2]
svm.tune.pred = predict.train(svm.tune, newdata=Test, type="prob")
svm.tune.pred.prob = svm.tune.pred[,2]
xGBM = .99
xRF = .01
xGLM = .0
xSVM = .0
xKNN = .0
Probability1 = (gbm.tune.pred.prob*xGBM+
rf.tune.pred.prob*xRF+
glm.tune.pred.prob*xGLM+
svm.tune.pred.prob*xSVM+
knn.tune.pred.prob*xKNN
)
MySubmission.Tuned = data.frame(UniqueID = NewsTest$UniqueID, Probability1)
write.csv(MySubmission.Tuned, "SubmissionTuned.csv", row.names=FALSE)
xGBM = .32
xRF = .29
xGLM = .12
xSVM = .27
xKNN = .0
Probability1 = (gbm.tune.pred.prob*xGBM+
rf.tune.pred.prob*xRF+
glm.tune.pred.prob*xGLM+
svm.tune.pred.prob*xSVM+
knn.tune.pred.prob*xKNN
)
MySubmission.Tuned = data.frame(UniqueID = NewsTest$UniqueID, Probability1)
write.csv(MySubmission.Tuned, "SubmissionTuned.csv", row.names=FALSE)
xGBM = .45
xRF = .55
xGLM = .00
xSVM = .00
xKNN = .0
Probability1 = (gbm.tune.pred.prob*xGBM+
rf.tune.pred.prob*xRF+
glm.tune.pred.prob*xGLM+
svm.tune.pred.prob*xSVM+
knn.tune.pred.prob*xKNN
)
MySubmission.Tuned = data.frame(UniqueID = NewsTest$UniqueID, Probability1)
write.csv(MySubmission.Tuned, "SubmissionTuned.csv", row.names=FALSE)
setwd("~/R/JohnsHopkins/PracticalMachineLearning")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.package(AppliedPredictiveModeling)
library(AppliedPredictiveModeling)
install.packages(AppliedPredictiveModeling)
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
View(training)
install.packages(c("caret", "manipulate", "NLP", "numDeriv", "pROC", "tm"))
install.packages("Hmisc")
?Hmisc
?cut2
??cut2
# Step-like pattern -> 4 categories
library(Hmisc)
cutCompressiveStrength <- cut2(training$CompressiveStrength, g=4)
summary(cutCompressiveStrength)
ggplot(data=training, aes(y=index, x=cutCompressiveStrength)) +
geom_boxplot() + geom_jitter(col="blue") + theme_bw()
library(ggplot)
cutCompressiveStrength <- cut2(training$CompressiveStrength, g=4)
summary(cutCompressiveStrength)
ggplot(data=training, aes(y=index, x=cutCompressiveStrength)) +
geom_boxplot() + geom_jitter(col="blue") + theme_bw()
xnames <- colnames(concrete)[1:8]
featurePlot(x=training[, xnames], y=training$CompressiveStrength, plot="pairs")
# No relation between the outcome and other variables
index <- seq_along(1:nrow(training))
ggplot(data=training, aes(x=index, y=CompressiveStrength)) + geom_point() +
theme_bw()
cutCompressiveStrength <- cut2(training$CompressiveStrength, g=4)
summary(cutCompressiveStrength)
ggplot(data=training, aes(y=index, x=cutCompressiveStrength)) +
geom_boxplot() + geom_jitter(col="blue") + theme_bw()
library(plyr)
splitOn <- cut2(training$Age, g=4)
splitOn <- mapvalues(splitOn,
from=levels(factor(splitOn)),
to=c("red", "blue", "yellow", "green"))
plot(training$CompressiveStrength, col=splitOn)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
summary(training$SuperPlasticizer)
summary(training)
log(0)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
preProc = preProcess((training[, xnames],
method = "pca",
verbose = T
newdata = training
))
preProc = preProcess((training[, xnames],
method = "pca",
verbose = T,
newdata = training
))
training[, xnames]
preProc = preProcess(x = training[, 1:8],
method = "pca",
verbose = T,
newdata = training
)
View(training)
View(training)
xvals = training[,grep('^IL', x = names(training) )]
View(xvals)
preProc = preProcess(x = xvals,
method = "pca",
verbose = T,
newdata = training
)
prePro $rotation
preProc$rotation
preProc = preProcess(x = xvals,
method = "pca",
thresh = 0.8
verbose = T,
outcome = training$diagnosis
)
preProc = preProcess(x = xvals,
method = "pca",
thresh = 0.8,
verbose = T,
outcome = training$diagnosis
)
preProc$rotation
preProc = preProcess(x = xvals,
method = "pca",
thresh = 0.9,
verbose = T,
outcome = training$diagnosis
)
preProc$rotation
set.seed(3433)
IL <- grep("^IL", colnames(training), value=TRUE)
ILpredictors <- predictors[, IL]
df <- data.frame(diagnosis, ILpredictors)
inTrain <- createDataPartition(df$diagnosis, p=3/4)[[1]]
training <- df[inTrain, ]
testing <- df[-inTrain, ]
modelFit <- train(diagnosis ~ ., method="glm", data=training)
predictions <- predict(modelFit, newdata=testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
acc1 <- C1$overall[1]
acc1 # Non-PCA Accuracy: 0.65
modelFit <- train(training$diagnosis ~ .,
method="glm",
preProcess="pca",
data=training,
trControl=trainControl(preProcOptions=list(thresh=0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
acc2 <- C2$overall[1]
acc2 # PCA Accuracy: 0.72
preProc = preProcess(x = xvals,
method = "pca",
thresh = 0.8,
verbose = T,
outcome = training$diagnosis
)
preProc$rotation
preProc = preProcess(x = xvals,
method = "pca",
thresh = 0.9,
verbose = T,
outcome = training$diagnosis
)
preProc$rotation
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
data <- segmentationOriginal
set.seed(125)
inTrain <- data$Case == "Train"
trainData <- data[inTrain, ]
testData <- data[!inTrain, ]
cartModel <- train(Class ~ ., data=trainData, method="rpart")
cartModel$finalModel
plot(cartModel$finalModel, uniform=T)
text(cartModel$finalModel, cex=0.8)
install.packages(pgmm)
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
View(newdata)
View(olive)
newdata = as.data.frame(t(colMeans(olive)))
treeModel <- train(Area ~ ., data=olive, method="rpart2")
treeModel
newdata <- as.data.frame(t(colMeans(olive)))
predict(treeModel, newdata) # 2.875
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
View(trainSA)
set.seed(13234)
SAHD = train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data=trainSA,
method="glm",
family="binomial")
SAHD
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
SAHD.pred = predict(SAHD, newdata = testSA)
SAHD.pred.TEST = predict(SAHD, newdata = testSA)
SAHD.pred.TEST = predict(SAHD, newdata = testSA)
SAHD.pred.TRAIN = predict(SAHD, data = trainSA)
SAHD.pred.TRAIN = predict(SAHD, trainSA)
missClass(trainSA$chd, SAHD.pred.TRAIN) # 0.2727273
# Test Set Misclassification rate
missClass(testSA$chd, SAHD.pred.TEST) # 0.3116883
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
set.seed(8484)
train = sample(1:dim(vowel.train)[1],size=dim(vowel.train)[1]/2,replace=F)
trainvowel = SAheart[train,]
testvowel = SAheart[-train,]
View(vowel.train)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.train)
head(vowel.test)
dim(vowel.train) # 528  11
dim(vowel.test) # 462  11
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
library(randomForest)
data(vowel.train)
data(vowel.test)
head(vowel.train)
head(vowel.test)
dim(vowel.train) # 528  11
dim(vowel.test) # 462  11
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
